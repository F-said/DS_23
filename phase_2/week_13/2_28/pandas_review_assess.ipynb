{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Basics\n",
    "\n",
    "Questions 1 through 10 rely on the following pseudo-dataframes. \n",
    "\n",
    "*warehouse*\n",
    "| prod_id | type      | price_kg | speciality | origin      |\n",
    "| ------- | --------- | -------- | ---------- | ----------- |\n",
    "| 0       | Robusta   | 2.586    | True       | Vietnam     |\n",
    "| 1       | Arabica   | 4.558    | False      | Brazil      |\n",
    "| 2       | Liberica  | 4.840    | False      | Phillipines |\n",
    "| 3       | Excelsa   | 4.840    | True       | India       |\n",
    "\n",
    "\n",
    "*orders*\n",
    "| ord_id | destination | kgs    | type     | prod_id |  comp_id |\n",
    "| ------ | ----------- | ------ | -------- | ------- | -------- |\n",
    "|  0     | USA         | 1000   | Robusta  | 0       | 0        |\n",
    "|  1     | USA         | 1000   | Arabica  | 1       | 0        |\n",
    "|  2     | USA         | 200    | Liberica | 2       | 1        |\n",
    "|  3     | Germany     | 1200   | Arabica  | 1       | 2        |\n",
    "|  4     | Germany     | 200    | Robusta  | 0       | 2        |\n",
    "|  5     | Japan       | 800    | Arabica  | 1       | 3        |\n",
    "|  6     | France      | 700    | Excelsa  | 3       | 4        |\n",
    "\n",
    "For each question, write out panda code that best accomplishes the listed requirements regarding these two tables. Assume that these two tables have been saved in two pandas dataframes called `ware_df` and `order_df` respectively.\n",
    "\n",
    "These tables do not exist anywhere in our repository. Just like before, you will write out code without the ability to test iteratively.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Utilize the pandas package to write a line of code that calculates the mean price of coffee.\n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [12/13 Intro to Pandas II Notes](https://github.com/The-Knowledge-House/DS_22/blob/main/phase_2/week_2/12_13/pandas_II_notes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Create a new column in the `warehouse` dataframe called `new_price` that increases all prices in the `price_kg` column by 5%.\n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [1/9 Intro to SQL Notes](https://github.com/The-Knowledge-House/DS_22/blob/main/phase_2/week_6/1_9/sql_intro_notes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "Create a new column in the `warehouse` dataframe called `adjusted_price` that increases all `speciality` coffee prices (from `new_price`) by 20%. \n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [1/9 Intro to SQL Notes](https://github.com/The-Knowledge-House/DS_22/blob/main/phase_2/week_6/1_9/sql_intro_notes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "Create a new dataframe called `orders_us` that displays only the orders whose destinations are `USA`.\n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [12/13 Intro to Pandas Lab](https://github.com/The-Knowledge-House/DS_22/blob/main/phase_2/week_2/12_13/pandas_II_notes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "In the current `orders` dataframe, the `kgs` column is expressed as a string. Write a line of code that will convert this column into an `int`.\n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [Pandas Docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "Let's say our `orders` dataframe is 1000 rows long. Some of these rows contain `NaN` values in the `destination` column. How would you print how many null values we have in this column?\n",
    "\n",
    "How would you drop these null rows from the `orders` dataframe?\n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [1/3 Pandas Cleaning Notes](https://github.com/The-Knowledge-House/DS_22/blob/main/phase_2/week_5/1_3/pandas_cleaning_notes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "Group your `orders` dataframe according to the `destination` column. Calculate the average `kgs` of an order going to this `destination`.\n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [12/13 Intro to Pandas Lab](https://github.com/The-Knowledge-House/DS_22/blob/main/phase_2/week_2/12_13/pandas_II_notes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8\n",
    "\n",
    "Group your `orders` dataframe according to the `type` column. Calculate the average `kgs` of an order from this `type`.\n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [12/13 Intro to Pandas Lab](https://github.com/The-Knowledge-House/DS_22/blob/main/phase_2/week_2/12_13/pandas_II_notes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9\n",
    "\n",
    "Write python code that joins these two dataframes together on coffee-type, creates a new column labeled `total_price` that calculates the total price of an order according to price_kg and adds $1000 for air-freight, and then orders this table from most expensive order to least.\n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [Pandas Docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10\n",
    "\n",
    "Write python code that joins these two dataframes together by coffee-type, and reveal the origin & destination of this order.\n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [Pandas Docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLAlchemy & Pandas\n",
    "\n",
    "Questions 11 - 20 will go over the basics of combining sqlalchemy with pandas. This will entail using the company data-tables saved in the `aws` postgres database, which include `jobs`, `salaries`, and `skills`. To get a better understanding of these tables, consult the company's [planning documents](https://drive.google.com/drive/folders/1z4EwdbyfUzf-FuRTJfVMaRSm5-R25viA).\n",
    "\n",
    "You will additionally use `pandas` to do some light exploration of these dataframes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11\n",
    "\n",
    "Import all 3 of your tables using sqlalchemy, and convert them into Python objects using the `auto_mapper`. Then, create 3 dataframes from the data pulled a session object. Be sure to dispose your engine after creating these dataframes.\n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [2/1 Intro to SQL Notes](https://github.com/The-Knowledge-House/DS_22/blob/main/phase_2/week_9/Lab/intermediate_sql_lab_notes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q12\n",
    "\n",
    "Join all 3 pandas dataframes that you created above on the primary key columns. Consult the planning docs to figure out how these tables were planned. Save this dataframe into a new variable.\n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [Pandas Docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q13\n",
    "\n",
    "Create a new dataframe that drops all null values in `salary_standardized` from this newly joined dataframe. Use this new dataframe to calculate the minimum offered `salary_standardized`, the maximum, the mean, and the standard deviation.\n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [1/3 Pandas Cleaning Notes](https://github.com/The-Knowledge-House/DS_22/blob/main/phase_2/week_5/1_3/pandas_cleaning_notes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q14\n",
    "\n",
    "Create a histogram to display the frequency of `salary_standardized` off of this new dataframe. \n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [2/6 Intro to Tableau](https://github.com/The-Knowledge-House/DS_22/blob/main/phase_2/week_10/2_6/intro_to_tableau_notes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q15\n",
    "\n",
    "Create a box-plot to display the distributions of `salary_standardized` off of this new dataframe. \n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [2/21 Data Analytics Notes](https://github.com/The-Knowledge-House/DS_22/blob/main/phase_2/week_12/2_21/data_analytics_notes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q16\n",
    "\n",
    "Create 3 new dataframes that filter for `New York, NY`, `San Fransisco, CA`, and `Atlanta, GA` from the `location` off of this new dataframe.\n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [12/13 Intro to Pandas Lab](https://github.com/The-Knowledge-House/DS_22/blob/main/phase_2/week_2/12_13/pandas_II_notes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q17\n",
    "\n",
    "Create 3 histograms using seaborn to plot the distribution of `salary_standardized` for each named city above using the dataframes you just created.\n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [2/21 Data Analytics Notes](https://github.com/The-Knowledge-House/DS_22/blob/main/phase_2/week_12/2_21/data_analytics_notes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q19\n",
    "\n",
    "Going back to Q15, what kind of distribution does this most likely represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Uniform\")\n",
    "print(\"Right Skewed\")\n",
    "print(\"Left Skewed\")\n",
    "print(\"Normal\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q20\n",
    "\n",
    "Calculate the `ks` test for normality on your `salary_standardized` column from the dataframe you used in `Q15`. What does this p-value tell you about this distribution? \n",
    "\n",
    "**Relevant Notes/Labs**\n",
    "* [2/21 Data Analytics Notes](https://github.com/The-Knowledge-House/DS_22/blob/main/phase_2/week_12/2_21/data_analytics_notes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
